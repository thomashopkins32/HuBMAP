{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2970a05",
   "metadata": {
    "papermill": {
     "duration": 0.005001,
     "end_time": "2023-07-25T12:02:36.060766",
     "exception": false,
     "start_time": "2023-07-25T12:02:36.055765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HuBMAP Release 2\n",
    "Thomas Hopkins\n",
    "\n",
    "## Overview\n",
    "\n",
    "Here is the first attempt at training a UNet for the HuBMAP competition.\n",
    "\n",
    "The code for this notebook is ported from my GitHub repository here: https://github.com/thomashopkins32/HuBMAP\n",
    "\n",
    "For loading `pycocotools` in Kaggle, you have to add the dataset in the sidebar on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0859e0d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:02:36.071741Z",
     "iopub.status.busy": "2023-07-25T12:02:36.071336Z",
     "iopub.status.idle": "2023-07-25T12:03:09.810324Z",
     "shell.execute_reply": "2023-07-25T12:03:09.809197Z"
    },
    "papermill": {
     "duration": 33.747729,
     "end_time": "2023-07-25T12:03:09.813123",
     "exception": false,
     "start_time": "2023-07-25T12:02:36.065394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/kaggle/input/pycocotools/': No such file or directory\n",
      "\u001b[31mERROR: Invalid requirement: '/kaggle/working/pycocotools/pycocotools-2.0.6'\n",
      "Hint: It looks like a path. File '/kaggle/working/pycocotools/pycocotools-2.0.6' does not exist.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cp -r /kaggle/input/pycocotools/ /kaggle/working/pycocotools\n",
    "!pip install /kaggle/working/pycocotools/pycocotools-2.0.6 --no-index --find-links=/kaggle/working/pycocotools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0bfd4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:09.827889Z",
     "iopub.status.busy": "2023-07-25T12:03:09.827366Z",
     "iopub.status.idle": "2023-07-25T12:03:22.185485Z",
     "shell.execute_reply": "2023-07-25T12:03:22.184474Z"
    },
    "papermill": {
     "duration": 12.368673,
     "end_time": "2023-07-25T12:03:22.188389",
     "exception": false,
     "start_time": "2023-07-25T12:03:09.819716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zlib\n",
    "import base64\n",
    "\n",
    "from pycocotools import _mask as coco_mask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import measure\n",
    "from skimage.draw import polygon\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db2d99",
   "metadata": {
    "papermill": {
     "duration": 0.008425,
     "end_time": "2023-07-25T12:03:22.208056",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.199631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities\n",
    "\n",
    "Here are some useful utilities that help in debugging, model training, and model evalutation.\n",
    "\n",
    "I wrote a custom version of mAP for fun so use at your own risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6763fe1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.230665Z",
     "iopub.status.busy": "2023-07-25T12:03:22.229945Z",
     "iopub.status.idle": "2023-07-25T12:03:22.573856Z",
     "shell.execute_reply": "2023-07-25T12:03:22.572668Z"
    },
    "papermill": {
     "duration": 0.357579,
     "end_time": "2023-07-25T12:03:22.576460",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.218881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def randrange(min_value, max_value, generator=None):\n",
    "    return (max_value - min_value) * torch.rand(1, generator=generator).item() + min_value\n",
    "\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return torch.count_nonzero(preds == labels) / preds.shape[0]\n",
    "\n",
    "\n",
    "def memory_usage_stats(model, optimizer, batch_size=1, device='cuda'):\n",
    "    print(f'Starting memory: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    model.to(device)\n",
    "    print(f'After model sent to {device}: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    for i in range(3):\n",
    "        sample = torch.randn((batch_size, 3, 512, 512))\n",
    "        print(f'Step {i}')\n",
    "        before = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        out = model(sample.to(device)).sum()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After forward pass: {after}')\n",
    "        print(f'Memory used by forward pass: {after - before}')\n",
    "        out.backward()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After backward pass: {after}')\n",
    "        optimizer.step()\n",
    "        print(f'After optimizer step: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def memory_usage_stats_grad_scaler(model, optimizer, batch_size=1, device='cuda'):\n",
    "    if device != 'cuda':\n",
    "        print('This function requires device to be \"cuda\".')\n",
    "        return\n",
    "    print(f'Starting memory: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    model.to(device)\n",
    "    print(f'After model sent to {device}: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for i in range(3):\n",
    "        sample = torch.randn((batch_size, 3, 512, 512))\n",
    "        print(f'Step {i}')\n",
    "        before = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(sample.to(device)).sum()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After forward pass: {after}')\n",
    "        print(f'Memory used by forward pass: {after - before}')\n",
    "        scaler.scale(out).backward()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After backward pass: {after}')\n",
    "        scaler.step(optimizer)\n",
    "        print(f'After optimizer step: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "        scaler.update()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def average_precision(prediction, target, iou_threshold=0.6):\n",
    "    '''\n",
    "    Computes the average precision (AP) for an instance segmentation task on a single image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : np.array\n",
    "        Boolean prediction mask\n",
    "    target : np.array\n",
    "        Boolean ground-truth mask\n",
    "    iou_threshold : float, optional\n",
    "        Threshold at which to count predictions as positive predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AP : float\n",
    "        Average precision score\n",
    "    '''\n",
    "    pred_regions, pred_region_count = measure.label(prediction, return_num=True)\n",
    "    target_regions, target_region_count = measure.label(target, return_num=True)\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for p in range(1, pred_region_count + 1):\n",
    "        pred_region_mask = pred_regions == p\n",
    "        max_iou = 0.0\n",
    "        for t in range(1, target_region_count + 1):\n",
    "            target_region_mask = target_regions == t\n",
    "            # Compute IoU this region pair\n",
    "            intersection = np.logical_and(pred_region_mask, target_region_mask)\n",
    "            union = np.logical_or(pred_region_mask, target_region_mask)\n",
    "            intersection_count = np.count_nonzero(intersection)\n",
    "            union_count = np.count_nonzero(union)\n",
    "            if intersection_count > 0 and union_count > 0:\n",
    "                iou = intersection_count / union_count\n",
    "                max_iou = max(max_iou, iou)\n",
    "        if max_iou > iou_threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    if (true_positives == 0 and false_positives == 0) or target_region_count == 0:\n",
    "        return 0.0\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / target_region_count\n",
    "\n",
    "    return precision * recall\n",
    "\n",
    "\n",
    "def mAP(predictions, targets, iou_threshold=0.6):\n",
    "    averages = 0.0\n",
    "    predictions = predictions.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    for p, t in zip(predictions, targets):\n",
    "        averages += average_precision(p, t, iou_threshold=iou_threshold)\n",
    "    return averages / len(predictions)\n",
    "\n",
    "\n",
    "def logits_to_blood_vessel_mask(logits):\n",
    "    return (torch.argmax(torch.softmax(logits, dim=1), dim=1) == 2).type(torch.long)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, model, train_loader, loss_func, optimizer, writer=None, data_transforms=False, device='cpu', **kwargs):\n",
    "    total_loss = 0.0\n",
    "    data_size = len(train_loader)\n",
    "    model.train()\n",
    "    for d in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        if data_transforms:\n",
    "            x = d['transformed_image']\n",
    "            y = d['transformed_mask']\n",
    "        else:\n",
    "            x = d['image']\n",
    "            y = d['mask']\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_func(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if writer:\n",
    "            with torch.no_grad():\n",
    "                predictions = logits_to_blood_vessel_mask(logits)\n",
    "                blood_vessel_gt = (y == 2).type(torch.long)\n",
    "                mAP_value = mAP(predictions, blood_vessel_gt, **kwargs)\n",
    "                writer.add_scalar(\"Loss/train\", loss.item(), global_step=epoch)\n",
    "                writer.add_scalar(\"mAP/train\", mAP_value, global_step=epoch)\n",
    "    \n",
    "    return total_loss / data_size\n",
    "\n",
    "\n",
    "def validate_one_epoch(epoch, model, valid_loader, loss_func, writer, data_transforms=False, device='cpu', **kwargs):\n",
    "    model.eval()\n",
    "    data_size = len(valid_loader)\n",
    "    total_loss = 0.0\n",
    "    total_mAP = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(valid_loader):\n",
    "            if data_transforms:\n",
    "                x = d['transformed_image']\n",
    "                y = d['transformed_mask']\n",
    "            else:\n",
    "                x = d['image']\n",
    "                y = d['mask']\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_func(logits, y)\n",
    "            predictions = logits_to_blood_vessel_mask(logits)\n",
    "            blood_vessel_gt = (y == 2).type(torch.long)\n",
    "            mAP_value = mAP(predictions, blood_vessel_gt, **kwargs)\n",
    "            writer.add_scalar(\"Loss/valid\", loss.item(), global_step=epoch)\n",
    "            writer.add_scalar(\"mAP/valid\", mAP_value, global_step=epoch)\n",
    "            total_loss += loss.item()\n",
    "            total_mAP += mAP_value\n",
    "\n",
    "    return total_loss / data_size, total_mAP / data_size\n",
    "    \n",
    "\n",
    "\n",
    "def oid_mask_encoding(mask):\n",
    "    '''\n",
    "    Taken from https://gist.github.com/pculliton/209398a2a52867580c6103e25e55d93c\n",
    "    using the competition page\n",
    "    '''\n",
    "    # check input mask --\n",
    "    if mask.dtype != bool:\n",
    "        raise ValueError(\n",
    "            \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n",
    "            mask.dtype)\n",
    "\n",
    "    mask = np.squeeze(mask)\n",
    "    if len(mask.shape) != 2:\n",
    "        raise ValueError(\n",
    "            \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n",
    "            mask.shape)\n",
    "\n",
    "    # convert input mask to expected COCO API input --\n",
    "    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "    mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "    mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "    # RLE encode mask --\n",
    "    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "    # compress and base64 encoding --\n",
    "    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "    base64_str = base64.b64encode(binary_str)\n",
    "    return base64_str\n",
    "\n",
    "\n",
    "def kaggle_prediction(image_id, logits):\n",
    "    '''\n",
    "    Encodes the raw output of the model into the submission format\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_id : str\n",
    "        Identifier of the predicted image\n",
    "    logits : torch.tensor\n",
    "        Raw output of the model with shape (3, 512, 512)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Submission entry with the headers (id, height, width, prediction_string)\n",
    "    '''\n",
    "\n",
    "    # keep raw prediction probabilities\n",
    "    preds = torch.softmax(logits, dim=0)\n",
    "\n",
    "    # convert blood vessel predictions to the mask\n",
    "    prediction = logits_to_blood_vessel_mask(logits.unsqueeze(0)).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # label predicted connected components\n",
    "    pred_regions, pred_region_count = measure.label(prediction, return_num=True)\n",
    "\n",
    "    prediction_string = ''\n",
    "    for p in range(1, pred_region_count + 1):\n",
    "        # get the coordinates of the current region\n",
    "        x, y = np.where(pred_regions == p)\n",
    "\n",
    "        # separate the binary mask\n",
    "        mask = np.zeros_like(pred_regions, dtype=bool)\n",
    "        mask[(x, y)] = True\n",
    "\n",
    "        # encode the separated mask\n",
    "        encoding = oid_mask_encoding(mask)\n",
    "\n",
    "        # average the prediction probabilities of the current region\n",
    "        region_prob = torch.mean(preds[2, x, y])\n",
    "\n",
    "        prediction_string += f'0 {region_prob.item()} {encoding.decode(\"utf-8\")} '\n",
    "    \n",
    "    return {\n",
    "        'id': image_id,\n",
    "        'height': 512,\n",
    "        'width': 512,\n",
    "        'prediction_string': prediction_string.strip()\n",
    "    }\n",
    "\n",
    "\n",
    "def save_model_checkpoint(path, epoch, model, optimizer, loss, scheduler=None):\n",
    "    checkpoint_dict = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'training_loss': loss,\n",
    "    }\n",
    "    if scheduler:\n",
    "        checkpoint_dict['scheduler_state_dict'] = scheduler.state_dict()\n",
    "    else:\n",
    "        checkpoint_dict['scheduler_state_dict'] = None\n",
    "    torch.save(checkpoint_dict, path)\n",
    "\n",
    "\n",
    "def load_model_checkpoint(path, model, optimizer=None, scheduler=None):\n",
    "    checkpoint = torch.load(path)\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d27729",
   "metadata": {
    "papermill": {
     "duration": 0.006372,
     "end_time": "2023-07-25T12:03:22.589593",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.583221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Architecture\n",
    "\n",
    "Here is the UNet architecture implementation.\n",
    "\n",
    "See the UNet paper for more information: https://arxiv.org/pdf/1505.04597v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e5561a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.603407Z",
     "iopub.status.busy": "2023-07-25T12:03:22.603096Z",
     "iopub.status.idle": "2023-07-25T12:03:22.619118Z",
     "shell.execute_reply": "2023-07-25T12:03:22.618200Z"
    },
    "papermill": {
     "duration": 0.025527,
     "end_time": "2023-07-25T12:03:22.621225",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.595698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        '''\n",
    "        Note: We pad the convolutions here since we know the input image size is always 512x512.\n",
    "        Otherwise we would do the \"Overlap-tile strategy\" presented in the u-net paper.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, xx):\n",
    "        return self.model(xx)\n",
    "\n",
    "\n",
    "class UNet2d(nn.Module):\n",
    "    ''' Paper: https://arxiv.org/pdf/1505.04597v1.pdf '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Down\n",
    "        self.conv1 = ConvBlock(3, 64)\n",
    "        self.conv2 = ConvBlock(64, 128)\n",
    "        self.conv3 = ConvBlock(128, 256)\n",
    "        self.conv4 = ConvBlock(256, 512)\n",
    "        self.conv5 = ConvBlock(512, 1024)\n",
    "\n",
    "        # Up\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv6 = ConvBlock(1024, 512)\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv7 = ConvBlock(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv8 = ConvBlock(256, 128)\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv9 = ConvBlock(128, 64)\n",
    "        self.conv_out = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, xx):\n",
    "        # Down\n",
    "        x1 = self.conv1(xx)\n",
    "        xx = self.max_pool(x1)\n",
    "        x2 = self.conv2(xx)\n",
    "        xx = self.max_pool(x2)\n",
    "        x3 = self.conv3(xx)\n",
    "        xx = self.max_pool(x3)\n",
    "        x4 = self.conv4(xx)\n",
    "        xx = self.max_pool(x4)\n",
    "        xx = self.conv5(xx)\n",
    "\n",
    "        # Up\n",
    "        xx = self.upconv1(xx)\n",
    "        xx = torch.cat((x4, xx), dim=1)\n",
    "        xx = self.conv6(xx)\n",
    "        xx = self.upconv2(xx)\n",
    "        xx = torch.cat((x3, xx), dim=1)\n",
    "        xx = self.conv7(xx)\n",
    "        xx = self.upconv3(xx)\n",
    "        xx = torch.cat((x2, xx), dim=1)\n",
    "        xx = self.conv8(xx)\n",
    "        xx = self.upconv4(xx)\n",
    "        xx = torch.cat((x1, xx), dim=1)\n",
    "        xx = self.conv9(xx)\n",
    "        return self.conv_out(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d3570",
   "metadata": {
    "papermill": {
     "duration": 0.00596,
     "end_time": "2023-07-25T12:03:22.633551",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.627591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Here is the dataset that loads the images, annotations, and peforms data augmentation.\n",
    "\n",
    "We allow for the prediction of the background (label 0), glomerulus (label 1), and blood vessels (label 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171776d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.647401Z",
     "iopub.status.busy": "2023-07-25T12:03:22.647129Z",
     "iopub.status.idle": "2023-07-25T12:03:22.667481Z",
     "shell.execute_reply": "2023-07-25T12:03:22.666679Z"
    },
    "papermill": {
     "duration": 0.029654,
     "end_time": "2023-07-25T12:03:22.669557",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.639903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class HuBMAP(Dataset):\n",
    "    ''' Training dataset for the HuBMAP Kaggle Competition '''\n",
    "    def __init__(self, data_dir=os.path.join('.', 'data'), submission=False, include_unsure=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.submission = submission\n",
    "        self.generator = torch.Generator()\n",
    "        self.generator.manual_seed(16)\n",
    "        self.image_ids = []\n",
    "        self.images = []\n",
    "        self.masks = [] # target structure\n",
    "\n",
    "        if not submission:\n",
    "            # Load in the training labels\n",
    "            with open(os.path.join(data_dir, 'polygons.jsonl'), 'r') as polygons_file:\n",
    "                polygons = list(polygons_file)\n",
    "            self.polygons = [json.loads(p) for p in polygons]\n",
    "\n",
    "            # Load all of the training images and annotations into memory\n",
    "            self.img_size = 512\n",
    "            print(\"Loading in images and converting annotations to polygon masks...\")\n",
    "            for poly in tqdm(self.polygons):\n",
    "                id = poly['id']\n",
    "                # Get image using id\n",
    "                image = Image.open(os.path.join(data_dir, 'train', f'{id}.tif'))\n",
    "                self.image_ids.append(id)\n",
    "                self.images.append(image)\n",
    "\n",
    "                # Get all of the different annotations for this image\n",
    "                ## A single image can have multiple annotations for each type\n",
    "                blood_vessel_coords = []\n",
    "                glomerulus_coords = []\n",
    "                unsure_coords = []\n",
    "                ## Load in the type and coordinates for each annotation\n",
    "                annotations = poly['annotations']\n",
    "                for ann in annotations:\n",
    "                    type = ann['type']\n",
    "                    assert len(ann['coordinates']) <= 1\n",
    "                    coordinates = ann['coordinates'][0]\n",
    "                    row_indices = [c[0] for c in coordinates] \n",
    "                    col_indices = [c[1] for c in coordinates]\n",
    "                    row_indices, col_indices = polygon(row_indices, col_indices, (self.img_size, self.img_size))\n",
    "                    coordinates = list(zip(row_indices, col_indices))\n",
    "                    if type == 'blood_vessel':\n",
    "                        blood_vessel_coords.append(coordinates)\n",
    "                    elif type == 'glomerulus':\n",
    "                        glomerulus_coords.append(coordinates)\n",
    "                    else:\n",
    "                        unsure_coords.append(coordinates) \n",
    "                mask = self.coordinates_to_mask([item for sublist in glomerulus_coords for item in sublist])\n",
    "                if include_unsure and unsure_coords is not None and len(unsure_coords) > 0:\n",
    "                    uns_coords = torch.tensor([item for sublist in unsure_coords for item in sublist])\n",
    "                    mask[uns_coords[:, 1], uns_coords[:, 0]] = 2\n",
    "                if blood_vessel_coords is not None and len(blood_vessel_coords) > 0:\n",
    "                    bv_coords = torch.tensor([item for sublist in blood_vessel_coords for item in sublist])\n",
    "                    # coordinates are (x, y) like a grid\n",
    "                    mask[bv_coords[:, 1], bv_coords[:, 0]] = 2\n",
    "                self.masks.append(mask)\n",
    "            print(\"Done.\")\n",
    "        else:\n",
    "            for image_file in os.listdir(os.path.join(data_dir, 'test')):\n",
    "                image = Image.open(os.path.join(data_dir, 'test', image_file))\n",
    "                id = image_file.split('.')[0]\n",
    "                self.image_ids.append(id)\n",
    "                self.images.append(image)\n",
    "                self.masks.append(torch.zeros((512, 512)))\n",
    "\n",
    "        self.test_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((512, 512), antialias=True)\n",
    "        ])\n",
    "\n",
    "    def transform(self, image, mask):\n",
    "        image = F.to_tensor(image)\n",
    "        resize = transforms.Resize(size=(512, 512), antialias=True)\n",
    "        image = resize(image)\n",
    "        mask = mask.unsqueeze(0)\n",
    "\n",
    "        # horizontal flip\n",
    "        if torch.rand(1, generator=self.generator).item() > 0.5:\n",
    "            image = F.hflip(image)\n",
    "            mask = F.hflip(mask)\n",
    "\n",
    "        # vertical flip\n",
    "        if torch.rand(1, generator=self.generator).item() > 0.5:\n",
    "            image = F.vflip(image)\n",
    "            mask = F.vflip(mask)\n",
    "        \n",
    "        # random affine (fill in with white pixels)\n",
    "        # see https://github.com/thomashopkins32/HuBMAP/issues/6#issuecomment-1656778016\n",
    "        angle = 0 # no rotation (weird borders)\n",
    "        shear = 0 # no shear (weird borders)\n",
    "        horizontal_translation_factor = randrange(0.0, 0.03, generator=self.generator)\n",
    "        ht_min = ceil(-512 * horizontal_translation_factor)\n",
    "        ht_max = floor(512 * horizontal_translation_factor)\n",
    "        vertical_translation_factor = randrange(0.0, 0.03, generator=self.generator)\n",
    "        vt_min = ceil(-512 * vertical_translation_factor)\n",
    "        vt_max = floor(-512 * vertical_translation_factor)\n",
    "        if ht_min < ht_max:\n",
    "            horizontal_translation = torch.randint(\n",
    "                ht_min,\n",
    "                ht_max,\n",
    "                (1,),\n",
    "                generator=self.generator).item()\n",
    "        else:\n",
    "            horizontal_translation = 0\n",
    "        if vt_min < vt_max:\n",
    "            vertical_translation = torch.randint(\n",
    "                vt_min,\n",
    "                vt_max,\n",
    "                (1,),\n",
    "                generator=self.generator).item()\n",
    "        else:\n",
    "            vertical_translation = 0\n",
    "        translation = (horizontal_translation, vertical_translation)\n",
    "        scale = randrange(1.0, 1.1, generator=self.generator)\n",
    "        fill = 1.0\n",
    "        image = F.affine(image, angle, translation, scale, shear, fill=fill)\n",
    "        mask = F.affine(mask, angle, translation, scale, shear, fill=0)\n",
    "\n",
    "        # brightness\n",
    "        brightness_factor = randrange(0.8, 1.2, generator=self.generator)\n",
    "        image = F.adjust_brightness(image, brightness_factor)\n",
    "\n",
    "        # contrast\n",
    "        contrast_factor = randrange(0.8, 1.2, generator=self.generator)\n",
    "        image = F.adjust_contrast(image, contrast_factor)\n",
    "\n",
    "        # saturation\n",
    "        saturation_factor = randrange(0.8, 1.2, generator=self.generator)\n",
    "        image = F.adjust_saturation(image, saturation_factor)\n",
    "\n",
    "        return image, mask.squeeze()\n",
    "\n",
    "    def coordinates_to_mask(self, coordinates):\n",
    "        if coordinates is None or coordinates == []:\n",
    "            return torch.zeros((self.img_size, self.img_size), dtype=torch.long)\n",
    "        coords = torch.tensor(coordinates)\n",
    "        mask = torch.zeros((self.img_size, self.img_size), dtype=torch.bool)\n",
    "        # coordinates are (x, y) like a grid\n",
    "        mask[coords[:, 1], coords[:, 0]] = True\n",
    "        return mask.type(torch.long)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.submission:\n",
    "            image = self.test_transforms(self.images[i])\n",
    "            mask = self.masks[i]\n",
    "            transformed_image = image\n",
    "            transformed_mask = mask\n",
    "        else:\n",
    "            transformed_image, transformed_mask = self.transform(self.images[i], self.masks[i])\n",
    "            image = self.test_transforms(self.images[i])\n",
    "            mask = self.masks[i]\n",
    "        return {\n",
    "            'id': self.image_ids[i],\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'transformed_image': transformed_image,\n",
    "            'transformed_mask': transformed_mask.type(torch.long),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61800df3",
   "metadata": {
    "papermill": {
     "duration": 0.005905,
     "end_time": "2023-07-25T12:03:22.681805",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.675900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CUDA Memory Usage\n",
    "\n",
    "It's important to see how much memory our model is using during training. Since this is a fairly large model with a large input (512x512 images), we need to know how large a batch size we can use on a single device. This will make training easier since our batch normalization will have better statistics the larger the batch size we use.\n",
    "\n",
    "\n",
    "Using a very low batch size could actually decrease performance when using batch normalization since it's statistics will be very noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf1601e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.694974Z",
     "iopub.status.busy": "2023-07-25T12:03:22.694709Z",
     "iopub.status.idle": "2023-07-25T12:03:22.698579Z",
     "shell.execute_reply": "2023-07-25T12:03:22.697659Z"
    },
    "papermill": {
     "duration": 0.012762,
     "end_time": "2023-07-25T12:03:22.700609",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.687847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = UNet2d()\n",
    "#optimizer = optim.Adam(model.parameters())\n",
    "#memory_usage_stats(model, optimizer, batch_size=9, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f80e0",
   "metadata": {
    "papermill": {
     "duration": 0.006014,
     "end_time": "2023-07-25T12:03:22.712745",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.706731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training\n",
    "\n",
    "Here is where we actually train the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5484fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.726192Z",
     "iopub.status.busy": "2023-07-25T12:03:22.725935Z",
     "iopub.status.idle": "2023-07-25T22:03:25.595240Z",
     "shell.execute_reply": "2023-07-25T22:03:25.592322Z"
    },
    "papermill": {
     "duration": 36002.956718,
     "end_time": "2023-07-25T22:03:25.675541",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.718823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/polygons.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(RNG)\n\u001b[1;32m     21\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter()\n\u001b[0;32m---> 22\u001b[0m dataset \u001b[39m=\u001b[39m HuBMAP(include_unsure\u001b[39m=\u001b[39;49mINCLUDE_UNSURE)\n\u001b[1;32m     23\u001b[0m generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator()\u001b[39m.\u001b[39mmanual_seed(RNG)\n\u001b[1;32m     24\u001b[0m train_data, valid_data \u001b[39m=\u001b[39m random_split(dataset, [\u001b[39m0.9\u001b[39m, \u001b[39m0.1\u001b[39m], generator\u001b[39m=\u001b[39mgenerator)\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mHuBMAP.__init__\u001b[0;34m(self, data_dir, submission, include_unsure)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasks \u001b[39m=\u001b[39m [] \u001b[39m# target structure\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m submission:\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Load in the training labels\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_dir, \u001b[39m'\u001b[39;49m\u001b[39mpolygons.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m polygons_file:\n\u001b[1;32m     15\u001b[0m         polygons \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(polygons_file)\n\u001b[1;32m     16\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolygons \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mloads(p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m polygons]\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/polygons.jsonl'"
     ]
    }
   ],
   "source": [
    "os.mkdir('checkpoints')\n",
    "# PARAMETERS\n",
    "RUN_NAME = 'release2_run1'\n",
    "BATCH_SIZE = 9\n",
    "LR = 3e-4\n",
    "WD = 0.0\n",
    "MOMENTUM = 0.0\n",
    "DATA_TRANSFORMATIONS = True\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "VALID_STEP = 5\n",
    "RNG = 32\n",
    "EPOCH_START = 0\n",
    "EPOCH_END = 200\n",
    "CHECKPOINT_STEP = 5\n",
    "CHECKPOINT_LOAD_PATH = None\n",
    "CHECKPOINT_SAVE_PATH = os.path.join('checkpoints', f'{RUN_NAME}.pt')\n",
    "INCLUDE_UNSURE = True\n",
    "\n",
    "torch.manual_seed(RNG)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "dataset = HuBMAP(include_unsure=INCLUDE_UNSURE)\n",
    "generator = torch.Generator().manual_seed(RNG)\n",
    "train_data, valid_data = random_split(dataset, [0.9, 0.1], generator=generator)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "valid_loader = DataLoader(valid_data, batch_size=1, shuffle=False, pin_memory=False)\n",
    "model = UNet2d().to(DEVICE)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max')\n",
    "if CHECKPOINT_LOAD_PATH:\n",
    "    EPOCH_START = load_model_checkpoint(CHECKPOINT_LOAD_PATH, model, optimizer, scheduler=scheduler) \n",
    "\n",
    "for e in tqdm(range(EPOCH_START + 1, EPOCH_END + 1)):\n",
    "    if e % VALID_STEP == 0:\n",
    "        loss = train_one_epoch(\n",
    "            e,\n",
    "            model,\n",
    "            train_loader,\n",
    "            loss_func,\n",
    "            optimizer,\n",
    "            writer=writer,\n",
    "            data_transforms=DATA_TRANSFORMATIONS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        val_loss, val_metric = validate_one_epoch(\n",
    "            e,\n",
    "            model,\n",
    "            valid_loader,\n",
    "            loss_func,\n",
    "            writer,\n",
    "            data_transforms=False,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        if scheduler:\n",
    "            scheduler.step(val_metric)\n",
    "    else:\n",
    "        loss = train_one_epoch(\n",
    "            e,\n",
    "            model,\n",
    "            train_loader,\n",
    "            loss_func,\n",
    "            optimizer, \n",
    "            data_transforms=DATA_TRANSFORMATIONS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "    if e % CHECKPOINT_STEP == 0:\n",
    "        save_model_checkpoint(CHECKPOINT_SAVE_PATH, e, model, optimizer, loss, scheduler=scheduler)\n",
    "    writer.add_scalar('gpu_memory_usage', torch.cuda.memory_allocated(DEVICE), global_step=e)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba33d67",
   "metadata": {
    "papermill": {
     "duration": 0.079246,
     "end_time": "2023-07-25T22:03:25.833491",
     "exception": false,
     "start_time": "2023-07-25T22:03:25.754245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission\n",
    "Here is where we predict on the test images for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30224af9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T22:03:25.995729Z",
     "iopub.status.busy": "2023-07-25T22:03:25.994689Z",
     "iopub.status.idle": "2023-07-25T22:03:26.146476Z",
     "shell.execute_reply": "2023-07-25T22:03:26.145420Z"
    },
    "papermill": {
     "duration": 0.237007,
     "end_time": "2023-07-25T22:03:26.149463",
     "exception": false,
     "start_time": "2023-07-25T22:03:25.912456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "RUN_NAME = 'release2_run1'\n",
    "BATCH_SIZE = 1\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "RNG = 32\n",
    "CHECKPOINT_LOAD_PATH = os.path.join('checkpoints', f'{RUN_NAME}.pt')\n",
    "\n",
    "torch.manual_seed(RNG)\n",
    "\n",
    "dataset = HuBMAP(data_dir=os.path.join('/kaggle', 'input', 'hubmap-hacking-the-human-vasculature'), submission=True)\n",
    "#generator = torch.Generator().manual_seed(RNG)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=False)\n",
    "#model = UNet2d().to(DEVICE)\n",
    "#if CHECKPOINT_LOAD_PATH:\n",
    "    #EPOCH_START = load_model_checkpoint(CHECKPOINT_LOAD_PATH, model)\n",
    "model.eval()\n",
    "\n",
    "submission_entries = pd.DataFrame(columns=['id', 'height', 'width', 'prediction_string'])\n",
    "for d in loader:\n",
    "    id = d['id']\n",
    "    x = d['image']\n",
    "    x = x.to(DEVICE)\n",
    "    logits = model(x)\n",
    "    for i in range(logits.shape[0]):\n",
    "        submission = kaggle_prediction(id, logits[i, :, :, :])\n",
    "        submission_df = pd.DataFrame(submission)\n",
    "        submission_entries = pd.concat((submission_entries, submission_df), axis=0)\n",
    "\n",
    "submission_entries.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36064.746083,
   "end_time": "2023-07-25T22:03:29.897373",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-25T12:02:25.151290",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d7d7a2fcff5e703a5794d475df409a360530cf46db42ac81a0f7eb7e8204921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
