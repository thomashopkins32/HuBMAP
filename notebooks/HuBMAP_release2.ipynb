{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HuBMAP Release 2\nThomas Hopkins\n\n## Overview\n\nHere is the first attempt at training a UNet for the HuBMAP competition.\n\nThe code for this notebook is ported from my GitHub repository here: https://github.com/thomashopkins32/HuBMAP","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/pycocotools/ /kaggle/working/pycocotools\n!pip install /kaggle/working/pycocotools/pycocotools-2.0.6 --no-index --find-links=/kaggle/working/pycocotools/","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:32:00.560706Z","iopub.execute_input":"2023-07-31T13:32:00.560980Z","iopub.status.idle":"2023-07-31T13:32:35.333854Z","shell.execute_reply.started":"2023-07-31T13:32:00.560954Z","shell.execute_reply":"2023-07-31T13:32:35.332602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport zlib\nimport base64\nfrom math import floor, ceil\n\nfrom pycocotools import _mask as coco_mask\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom skimage import measure\nfrom skimage.draw import polygon\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as F\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:32:35.336065Z","iopub.execute_input":"2023-07-31T13:32:35.336390Z","iopub.status.idle":"2023-07-31T13:32:46.504523Z","shell.execute_reply.started":"2023-07-31T13:32:35.336360Z","shell.execute_reply":"2023-07-31T13:32:46.503533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utilities\n\nHere are some useful utilities that help in debugging, model training, and model evalutation.\n\nI wrote a custom version of mAP for fun so use at your own risk.","metadata":{}},{"cell_type":"code","source":"def randrange(min_value, max_value, generator=None):\n    return (max_value - min_value) * torch.rand(1, generator=generator).item() + min_value\n\n\ndef accuracy(logits, labels):\n    preds = torch.argmax(logits, dim=1)\n    return torch.count_nonzero(preds == labels) / preds.shape[0]\n\n\ndef memory_usage_stats(model, optimizer, batch_size=1, device='cuda'):\n    print(f'Starting memory: {torch.cuda.memory_allocated(device) * 1e-6}')\n    model.to(device)\n    print(f'After model sent to {device}: {torch.cuda.memory_allocated(device) * 1e-6}')\n    for i in range(3):\n        sample = torch.randn((batch_size, 3, 512, 512))\n        print(f'Step {i}')\n        before = torch.cuda.memory_allocated(device) * 1e-6\n        out = model(sample.to(device)).sum()\n        after = torch.cuda.memory_allocated(device) * 1e-6\n        print(f'After forward pass: {after}')\n        print(f'Memory used by forward pass: {after - before}')\n        out.backward()\n        after = torch.cuda.memory_allocated(device) * 1e-6\n        print(f'After backward pass: {after}')\n        optimizer.step()\n        print(f'After optimizer step: {torch.cuda.memory_allocated(device) * 1e-6}')\n    torch.cuda.empty_cache()\n\n\ndef memory_usage_stats_grad_scaler(model, optimizer, batch_size=1, device='cuda'):\n    if device != 'cuda':\n        print('This function requires device to be \"cuda\".')\n        return\n    print(f'Starting memory: {torch.cuda.memory_allocated(device) * 1e-6}')\n    model.to(device)\n    print(f'After model sent to {device}: {torch.cuda.memory_allocated(device) * 1e-6}')\n    scaler = torch.cuda.amp.GradScaler()\n    for i in range(3):\n        sample = torch.randn((batch_size, 3, 512, 512))\n        print(f'Step {i}')\n        before = torch.cuda.memory_allocated(device) * 1e-6\n        with torch.cuda.amp.autocast(dtype=torch.float16):\n            out = model(sample.to(device)).sum()\n        after = torch.cuda.memory_allocated(device) * 1e-6\n        print(f'After forward pass: {after}')\n        print(f'Memory used by forward pass: {after - before}')\n        scaler.scale(out).backward()\n        after = torch.cuda.memory_allocated(device) * 1e-6\n        print(f'After backward pass: {after}')\n        scaler.step(optimizer)\n        print(f'After optimizer step: {torch.cuda.memory_allocated(device) * 1e-6}')\n        scaler.update()\n    torch.cuda.empty_cache()\n\n\ndef average_precision(prediction, target, iou_threshold=0.6):\n    '''\n    Computes the average precision (AP) for an instance segmentation task on a single image.\n\n    Parameters\n    ----------\n    prediction : np.array\n        Boolean prediction mask\n    target : np.array\n        Boolean ground-truth mask\n    iou_threshold : float, optional\n        Threshold at which to count predictions as positive predictions\n\n    Returns\n    -------\n    AP : float\n        Average precision score\n    '''\n    pred_regions, pred_region_count = measure.label(prediction, return_num=True)\n    target_regions, target_region_count = measure.label(target, return_num=True)\n    true_positives = 0\n    false_positives = 0\n    for p in range(1, pred_region_count + 1):\n        pred_region_mask = pred_regions == p\n        max_iou = 0.0\n        for t in range(1, target_region_count + 1):\n            target_region_mask = target_regions == t\n            # Compute IoU this region pair\n            intersection = np.logical_and(pred_region_mask, target_region_mask)\n            union = np.logical_or(pred_region_mask, target_region_mask)\n            intersection_count = np.count_nonzero(intersection)\n            union_count = np.count_nonzero(union)\n            if intersection_count > 0 and union_count > 0:\n                iou = intersection_count / union_count\n                max_iou = max(max_iou, iou)\n        if max_iou > iou_threshold:\n            true_positives += 1\n        else:\n            false_positives += 1\n    if (true_positives == 0 and false_positives == 0) or target_region_count == 0:\n        return 0.0\n    precision = true_positives / (true_positives + false_positives)\n    recall = true_positives / target_region_count\n\n    return precision * recall\n\n\ndef mAP(predictions, targets, iou_threshold=0.6):\n    averages = 0.0\n    predictions = predictions.cpu().detach().numpy()\n    targets = targets.cpu().detach().numpy()\n    for p, t in zip(predictions, targets):\n        averages += average_precision(p, t, iou_threshold=iou_threshold)\n    return averages / len(predictions)\n\n\ndef logits_to_blood_vessel_mask(logits):\n    return (torch.argmax(torch.softmax(logits, dim=1), dim=1) == 2).type(torch.long)\n\n\ndef train_one_epoch(epoch, model, train_loader, loss_func, optimizer, writer=None, data_transforms=False, device='cpu', **kwargs):\n    total_loss = 0.0\n    data_size = len(train_loader)\n    model.train()\n    for d in train_loader:\n        optimizer.zero_grad()\n        if data_transforms:\n            x = d['transformed_image']\n            y = d['transformed_mask']\n        else:\n            x = d['image']\n            y = d['mask']\n        x = x.to(device)\n        y = y.to(device)\n        logits = model(x)\n        loss = loss_func(logits, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        if writer:\n            with torch.no_grad():\n                predictions = logits_to_blood_vessel_mask(logits)\n                blood_vessel_gt = (y == 2).type(torch.long)\n                mAP_value = mAP(predictions, blood_vessel_gt, **kwargs)\n                writer.add_scalar(\"Loss/train\", loss.item(), global_step=epoch)\n                writer.add_scalar(\"mAP/train\", mAP_value, global_step=epoch)\n    \n    return total_loss / data_size\n\n\ndef validate_one_epoch(epoch, model, valid_loader, loss_func, writer, data_transforms=False, device='cpu', **kwargs):\n    model.eval()\n    data_size = len(valid_loader)\n    total_loss = 0.0\n    total_mAP = 0.0\n    with torch.no_grad():\n        for i, d in enumerate(valid_loader):\n            if data_transforms:\n                x = d['transformed_image']\n                y = d['transformed_mask']\n            else:\n                x = d['image']\n                y = d['mask']\n            x = x.to(device)\n            y = y.to(device)\n            logits = model(x)\n            loss = loss_func(logits, y)\n            predictions = logits_to_blood_vessel_mask(logits)\n            blood_vessel_gt = (y == 2).type(torch.long)\n            mAP_value = mAP(predictions, blood_vessel_gt, **kwargs)\n            writer.add_scalar(\"Loss/valid\", loss.item(), global_step=epoch)\n            writer.add_scalar(\"mAP/valid\", mAP_value, global_step=epoch)\n            total_loss += loss.item()\n            total_mAP += mAP_value\n\n    return total_loss / data_size, total_mAP / data_size\n    \n\n\ndef oid_mask_encoding(mask):\n    '''\n    Taken from https://gist.github.com/pculliton/209398a2a52867580c6103e25e55d93c\n    using the competition page\n    '''\n    # check input mask --\n    if mask.dtype != bool:\n        raise ValueError(\n            \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n            mask.dtype)\n\n    mask = np.squeeze(mask)\n    if len(mask.shape) != 2:\n        raise ValueError(\n            \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n            mask.shape)\n\n    # convert input mask to expected COCO API input --\n    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n    mask_to_encode = mask_to_encode.astype(np.uint8)\n    mask_to_encode = np.asfortranarray(mask_to_encode)\n\n    # RLE encode mask --\n    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n\n    # compress and base64 encoding --\n    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n    base64_str = base64.b64encode(binary_str)\n    return base64_str\n\n\ndef kaggle_prediction(image_id, logits):\n    '''\n    Encodes the raw output of the model into the submission format\n    \n    Parameters\n    ----------\n    image_id : str\n        Identifier of the predicted image\n    logits : torch.tensor\n        Raw output of the model with shape (3, 512, 512)\n    \n    Returns\n    -------\n    dict\n        Submission entry with the headers (id, height, width, prediction_string)\n    '''\n\n    # keep raw prediction probabilities\n    preds = torch.softmax(logits, dim=0)\n\n    # convert blood vessel predictions to the mask\n    prediction = logits_to_blood_vessel_mask(logits.unsqueeze(0)).squeeze().detach().cpu().numpy()\n\n    # label predicted connected components\n    pred_regions, pred_region_count = measure.label(prediction, return_num=True)\n\n    prediction_string = ''\n    for p in range(1, pred_region_count + 1):\n        # get the coordinates of the current region\n        x, y = np.where(pred_regions == p)\n\n        # separate the binary mask\n        mask = np.zeros_like(pred_regions, dtype=bool)\n        mask[(x, y)] = True\n\n        # encode the separated mask\n        encoding = oid_mask_encoding(mask)\n\n        # average the prediction probabilities of the current region\n        region_prob = torch.mean(preds[2, x, y])\n\n        prediction_string += f'0 {region_prob.item()} {encoding.decode(\"utf-8\")} '\n    \n    return {\n        'id': image_id,\n        'height': 512,\n        'width': 512,\n        'prediction_string': prediction_string.strip()\n    }\n\n\ndef save_model_checkpoint(path, epoch, model, optimizer, loss, scheduler=None):\n    checkpoint_dict = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'training_loss': loss,\n    }\n    if scheduler:\n        checkpoint_dict['scheduler_state_dict'] = scheduler.state_dict()\n    else:\n        checkpoint_dict['scheduler_state_dict'] = None\n    torch.save(checkpoint_dict, path)\n\n\ndef load_model_checkpoint(path, model, optimizer=None, scheduler=None):\n    checkpoint = torch.load(path)\n    epoch = checkpoint['epoch']\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    if scheduler and checkpoint['scheduler_state_dict']:\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    return epoch","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:32:46.506025Z","iopub.execute_input":"2023-07-31T13:32:46.506690Z","iopub.status.idle":"2023-07-31T13:32:46.800741Z","shell.execute_reply.started":"2023-07-31T13:32:46.506654Z","shell.execute_reply":"2023-07-31T13:32:46.799695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Architecture\n\nHere is the UNet architecture implementation.\n\nSee the UNet paper for more information: https://arxiv.org/pdf/1505.04597v1.pdf","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        '''\n        Note: We pad the convolutions here since we know the input image size is always 512x512.\n        Otherwise we would do the \"Overlap-tile strategy\" presented in the u-net paper.\n        '''\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, xx):\n        return self.model(xx)\n\n\nclass UNet2d(nn.Module):\n    ''' Paper: https://arxiv.org/pdf/1505.04597v1.pdf '''\n    def __init__(self):\n        super().__init__()\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n\n        # Down\n        self.conv1 = ConvBlock(3, 64)\n        self.conv2 = ConvBlock(64, 128)\n        self.conv3 = ConvBlock(128, 256)\n        self.conv4 = ConvBlock(256, 512)\n        self.conv5 = ConvBlock(512, 1024)\n\n        # Up\n        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.conv6 = ConvBlock(1024, 512)\n        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.conv7 = ConvBlock(512, 256)\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.conv8 = ConvBlock(256, 128)\n        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.conv9 = ConvBlock(128, 64)\n        self.conv_out = nn.Conv2d(64, 3, kernel_size=1)\n\n    def forward(self, xx):\n        # Down\n        x1 = self.conv1(xx)\n        xx = self.max_pool(x1)\n        x2 = self.conv2(xx)\n        xx = self.max_pool(x2)\n        x3 = self.conv3(xx)\n        xx = self.max_pool(x3)\n        x4 = self.conv4(xx)\n        xx = self.max_pool(x4)\n        xx = self.conv5(xx)\n\n        # Up\n        xx = self.upconv1(xx)\n        xx = torch.cat((x4, xx), dim=1)\n        xx = self.conv6(xx)\n        xx = self.upconv2(xx)\n        xx = torch.cat((x3, xx), dim=1)\n        xx = self.conv7(xx)\n        xx = self.upconv3(xx)\n        xx = torch.cat((x2, xx), dim=1)\n        xx = self.conv8(xx)\n        xx = self.upconv4(xx)\n        xx = torch.cat((x1, xx), dim=1)\n        xx = self.conv9(xx)\n        return self.conv_out(xx)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:32:46.804177Z","iopub.execute_input":"2023-07-31T13:32:46.804580Z","iopub.status.idle":"2023-07-31T13:32:46.828087Z","shell.execute_reply.started":"2023-07-31T13:32:46.804547Z","shell.execute_reply":"2023-07-31T13:32:46.827067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\n\nHere is the dataset that loads the images, annotations, and peforms data augmentation.\n\nWe allow for the prediction of the background (label 0), glomerulus (label 1), and blood vessels (label 2).","metadata":{}},{"cell_type":"code","source":"class HuBMAP(Dataset):\n    ''' Training dataset for the HuBMAP Kaggle Competition '''\n    def __init__(self, data_dir=os.path.join('.', 'data'), submission=False, include_unsure=False):\n        self.data_dir = data_dir\n        self.submission = submission\n        self.generator = torch.Generator()\n        self.generator.manual_seed(16)\n        self.image_ids = []\n        self.images = []\n        self.masks = [] # target structure\n\n        if not submission:\n            # Load in the training labels\n            with open(os.path.join(data_dir, 'polygons.jsonl'), 'r') as polygons_file:\n                polygons = list(polygons_file)\n            self.polygons = [json.loads(p) for p in polygons]\n\n            # Load all of the training images and annotations into memory\n            self.img_size = 512\n            print(\"Loading in images and converting annotations to polygon masks...\")\n            for poly in tqdm(self.polygons):\n                id = poly['id']\n                # Get image using id\n                image = Image.open(os.path.join(data_dir, 'train', f'{id}.tif'))\n                self.image_ids.append(id)\n                self.images.append(image)\n\n                # Get all of the different annotations for this image\n                ## A single image can have multiple annotations for each type\n                blood_vessel_coords = []\n                glomerulus_coords = []\n                unsure_coords = []\n                ## Load in the type and coordinates for each annotation\n                annotations = poly['annotations']\n                for ann in annotations:\n                    type = ann['type']\n                    assert len(ann['coordinates']) <= 1\n                    coordinates = ann['coordinates'][0]\n                    row_indices = [c[0] for c in coordinates] \n                    col_indices = [c[1] for c in coordinates]\n                    row_indices, col_indices = polygon(row_indices, col_indices, (self.img_size, self.img_size))\n                    coordinates = list(zip(row_indices, col_indices))\n                    if type == 'blood_vessel':\n                        blood_vessel_coords.append(coordinates)\n                    elif type == 'glomerulus':\n                        glomerulus_coords.append(coordinates)\n                    else:\n                        unsure_coords.append(coordinates) \n                mask = self.coordinates_to_mask([item for sublist in glomerulus_coords for item in sublist])\n                if include_unsure and unsure_coords is not None and len(unsure_coords) > 0:\n                    uns_coords = torch.tensor([item for sublist in unsure_coords for item in sublist])\n                    mask[uns_coords[:, 1], uns_coords[:, 0]] = 2\n                if blood_vessel_coords is not None and len(blood_vessel_coords) > 0:\n                    bv_coords = torch.tensor([item for sublist in blood_vessel_coords for item in sublist])\n                    # coordinates are (x, y) like a grid\n                    mask[bv_coords[:, 1], bv_coords[:, 0]] = 2\n                self.masks.append(mask)\n            print(\"Done.\")\n        else:\n            for image_file in os.listdir(os.path.join(data_dir, 'test')):\n                image = Image.open(os.path.join(data_dir, 'test', image_file))\n                id = image_file.split('.')[0]\n                self.image_ids.append(id)\n                self.images.append(image)\n                self.masks.append(torch.zeros((512, 512)))\n\n        self.test_transforms = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((512, 512), antialias=True)\n        ])\n\n    def transform(self, image, mask):\n        image = F.to_tensor(image)\n        resize = transforms.Resize(size=(512, 512), antialias=True)\n        image = resize(image)\n        mask = mask.unsqueeze(0)\n\n        # horizontal flip\n        if torch.rand(1, generator=self.generator).item() > 0.5:\n            image = F.hflip(image)\n            mask = F.hflip(mask)\n\n        # vertical flip\n        if torch.rand(1, generator=self.generator).item() > 0.5:\n            image = F.vflip(image)\n            mask = F.vflip(mask)\n        \n        # random affine (fill in with white pixels)\n        # see https://github.com/thomashopkins32/HuBMAP/issues/6#issuecomment-1656778016\n        angle = 0 # no rotation (weird borders)\n        shear = 0 # no shear (weird borders)\n        horizontal_translation_factor = randrange(0.0, 0.03, generator=self.generator)\n        ht_min = ceil(-512 * horizontal_translation_factor)\n        ht_max = floor(512 * horizontal_translation_factor)\n        vertical_translation_factor = randrange(0.0, 0.03, generator=self.generator)\n        vt_min = ceil(-512 * vertical_translation_factor)\n        vt_max = floor(-512 * vertical_translation_factor)\n        if ht_min < ht_max:\n            horizontal_translation = torch.randint(\n                ht_min,\n                ht_max,\n                (1,),\n                generator=self.generator).item()\n        else:\n            horizontal_translation = 0\n        if vt_min < vt_max:\n            vertical_translation = torch.randint(\n                vt_min,\n                vt_max,\n                (1,),\n                generator=self.generator).item()\n        else:\n            vertical_translation = 0\n        translation = (horizontal_translation, vertical_translation)\n        scale = randrange(1.0, 1.1, generator=self.generator)\n        fill = 1.0\n        image = F.affine(image, angle, translation, scale, shear, fill=fill)\n        mask = F.affine(mask, angle, translation, scale, shear, fill=0)\n\n        # brightness\n        brightness_factor = randrange(0.8, 1.2, generator=self.generator)\n        image = F.adjust_brightness(image, brightness_factor)\n\n        # contrast\n        contrast_factor = randrange(0.8, 1.2, generator=self.generator)\n        image = F.adjust_contrast(image, contrast_factor)\n\n        # saturation\n        saturation_factor = randrange(0.8, 1.2, generator=self.generator)\n        image = F.adjust_saturation(image, saturation_factor)\n\n        return image, mask.squeeze()\n\n    def coordinates_to_mask(self, coordinates):\n        if coordinates is None or coordinates == []:\n            return torch.zeros((self.img_size, self.img_size), dtype=torch.long)\n        coords = torch.tensor(coordinates)\n        mask = torch.zeros((self.img_size, self.img_size), dtype=torch.bool)\n        # coordinates are (x, y) like a grid\n        mask[coords[:, 1], coords[:, 0]] = True\n        return mask.type(torch.long)\n\n    def __getitem__(self, i):\n        if self.submission:\n            image = self.test_transforms(self.images[i])\n            mask = self.masks[i]\n            transformed_image = image\n            transformed_mask = mask\n        else:\n            transformed_image, transformed_mask = self.transform(self.images[i], self.masks[i])\n            image = self.test_transforms(self.images[i])\n            mask = self.masks[i]\n        return {\n            'id': self.image_ids[i],\n            'image': image,\n            'mask': mask,\n            'transformed_image': transformed_image,\n            'transformed_mask': transformed_mask.type(torch.long),\n        }\n\n    def __len__(self):\n        return len(self.images)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:32:46.829534Z","iopub.execute_input":"2023-07-31T13:32:46.829972Z","iopub.status.idle":"2023-07-31T13:32:46.863946Z","shell.execute_reply.started":"2023-07-31T13:32:46.829941Z","shell.execute_reply":"2023-07-31T13:32:46.863005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CUDA Memory Usage\n\nIt's important to see how much memory our model is using during training. Since this is a fairly large model with a large input (512x512 images), we need to know how large a batch size we can use on a single device. This will make training easier since our batch normalization will have better statistics the larger the batch size we use.\n\n\nUsing a very low batch size could actually decrease performance when using batch normalization since it's statistics will be very noisy.","metadata":{}},{"cell_type":"code","source":"#model = UNet2d()\n#optimizer = optim.Adam(model.parameters())\n#memory_usage_stats(model, optimizer, batch_size=9, device='cuda')","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:32:46.865076Z","iopub.execute_input":"2023-07-31T13:32:46.865601Z","iopub.status.idle":"2023-07-31T13:32:46.878890Z","shell.execute_reply.started":"2023-07-31T13:32:46.865571Z","shell.execute_reply":"2023-07-31T13:32:46.877831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nHere is where we actually train the UNet.","metadata":{}},{"cell_type":"code","source":"if not os.path.exists('checkpoints'):\n    os.mkdir('checkpoints')\n# PARAMETERS\nRUN_NAME = 'release2_run1'\nBATCH_SIZE = 9\nLR = 3e-5\nWD = 0.0\nMOMENTUM = 0.0\nDATA_TRANSFORMATIONS = True\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nVALID_STEP = 5\nRNG = 32\nEPOCH_START = 0\nEPOCH_END = 350\nCHECKPOINT_STEP = 5\nCHECKPOINT_LOAD_PATH = os.path.join('/kaggle', 'input', 'hubmap-release2-run1', f'{RUN_NAME}.pt')\nCHECKPOINT_SAVE_PATH = os.path.join('checkpoints', f'{RUN_NAME}.pt')\nINCLUDE_UNSURE = True\n\ntorch.manual_seed(RNG)\n\nwriter = SummaryWriter()\ndataset = HuBMAP(data_dir=os.path.join('/kaggle', 'input', 'hubmap-hacking-the-human-vasculature'),\n                 include_unsure=INCLUDE_UNSURE)\ngenerator = torch.Generator().manual_seed(RNG)\ntrain_data, valid_data = random_split(dataset, [0.9, 0.1], generator=generator)\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\nvalid_loader = DataLoader(valid_data, batch_size=1, shuffle=False, pin_memory=False)\nmodel = UNet2d().to(DEVICE)\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR)\n#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max')\nscheduler = None\nif CHECKPOINT_LOAD_PATH:\n    EPOCH_START = load_model_checkpoint(CHECKPOINT_LOAD_PATH, model, optimizer, scheduler=scheduler) \n\nfor e in tqdm(range(EPOCH_START + 1, EPOCH_END + 1)):\n    if e % VALID_STEP == 0:\n        loss = train_one_epoch(\n            e,\n            model,\n            train_loader,\n            loss_func,\n            optimizer,\n            writer=writer,\n            data_transforms=DATA_TRANSFORMATIONS,\n            device=DEVICE\n        )\n        val_loss, val_metric = validate_one_epoch(\n            e,\n            model,\n            valid_loader,\n            loss_func,\n            writer,\n            data_transforms=False,\n            device=DEVICE\n        )\n        if scheduler:\n            scheduler.step(val_metric)\n    else:\n        loss = train_one_epoch(\n            e,\n            model,\n            train_loader,\n            loss_func,\n            optimizer, \n            data_transforms=DATA_TRANSFORMATIONS,\n            device=DEVICE\n        )\n    if e % CHECKPOINT_STEP == 0:\n        save_model_checkpoint(CHECKPOINT_SAVE_PATH, e, model, optimizer, loss, scheduler=scheduler)\n    writer.add_scalar('gpu_memory_usage', torch.cuda.memory_allocated(DEVICE), global_step=e)\nwriter.close()","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:32:46.880303Z","iopub.execute_input":"2023-07-31T13:32:46.880975Z","iopub.status.idle":"2023-07-31T13:38:02.600815Z","shell.execute_reply.started":"2023-07-31T13:32:46.880943Z","shell.execute_reply":"2023-07-31T13:38:02.599012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\nHere is where we predict on the test images for submission.","metadata":{}},{"cell_type":"code","source":"# PARAMETERS\nRUN_NAME = 'release2_run1'\nBATCH_SIZE = 1\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nRNG = 32\nCHECKPOINT_LOAD_PATH = os.path.join('checkpoints', f'{RUN_NAME}.pt')\n\ntorch.manual_seed(RNG)\n\ndataset = HuBMAP(data_dir=os.path.join('/kaggle', 'input', 'hubmap-hacking-the-human-vasculature'), submission=True)\n#generator = torch.Generator().manual_seed(RNG)\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=False)\n#model = UNet2d().to(DEVICE)\n#if CHECKPOINT_LOAD_PATH:\n    #EPOCH_START = load_model_checkpoint(CHECKPOINT_LOAD_PATH, model)\nmodel.eval()\n\nsubmission_entries = pd.DataFrame(columns=['id', 'height', 'width', 'prediction_string'])\nfor d in loader:\n    id = d['id']\n    x = d['image']\n    x = x.to(DEVICE)\n    logits = model(x)\n    for i in range(logits.shape[0]):\n        submission = kaggle_prediction(id, logits[i, :, :, :])\n        submission_df = pd.DataFrame(submission)\n        submission_entries = pd.concat((submission_entries, submission_df), axis=0)\n\nsubmission_entries.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:38:02.601905Z","iopub.status.idle":"2023-07-31T13:38:02.602661Z","shell.execute_reply.started":"2023-07-31T13:38:02.602412Z","shell.execute_reply":"2023-07-31T13:38:02.602436Z"},"trusted":true},"execution_count":null,"outputs":[]}]}