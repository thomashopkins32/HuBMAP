{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2970a05",
   "metadata": {
    "papermill": {
     "duration": 0.005001,
     "end_time": "2023-07-25T12:02:36.060766",
     "exception": false,
     "start_time": "2023-07-25T12:02:36.055765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HuBMAP Release 1\n",
    "Thomas Hopkins\n",
    "\n",
    "## Overview\n",
    "\n",
    "Here is the first attempt at training a UNet for the HuBMAP competition.\n",
    "\n",
    "The code for this notebook is ported from my GitHub repository here: https://github.com/thomashopkins32/HuBMAP\n",
    "\n",
    "For loading `pycocotools` in Kaggle, you have to add the dataset in the sidebar on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0859e0d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:02:36.071741Z",
     "iopub.status.busy": "2023-07-25T12:02:36.071336Z",
     "iopub.status.idle": "2023-07-25T12:03:09.810324Z",
     "shell.execute_reply": "2023-07-25T12:03:09.809197Z"
    },
    "papermill": {
     "duration": 33.747729,
     "end_time": "2023-07-25T12:03:09.813123",
     "exception": false,
     "start_time": "2023-07-25T12:02:36.065394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/working/pycocotools/\r\n",
      "Processing ./pycocotools/pycocotools-2.0.6\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools==2.0.6) (3.6.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools==2.0.6) (1.23.5)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6) (1.0.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6) (4.39.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0.6) (1.16.0)\r\n",
      "Building wheels for collected packages: pycocotools\r\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=93516 sha256=59860fa514f3a621c08b5219914a3cf3be85f002357fe5ee21274cd602e5546a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/be/1a/87ffe91b7dc6435df82208f620dbdcc654f3144d97550df6d8\r\n",
      "Successfully built pycocotools\r\n",
      "Installing collected packages: pycocotools\r\n",
      "Successfully installed pycocotools-2.0.6\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cp -r /kaggle/input/pycocotools/ /kaggle/working/pycocotools\n",
    "!pip install /kaggle/working/pycocotools/pycocotools-2.0.6 --no-index --find-links=/kaggle/working/pycocotools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0bfd4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:09.827889Z",
     "iopub.status.busy": "2023-07-25T12:03:09.827366Z",
     "iopub.status.idle": "2023-07-25T12:03:22.185485Z",
     "shell.execute_reply": "2023-07-25T12:03:22.184474Z"
    },
    "papermill": {
     "duration": 12.368673,
     "end_time": "2023-07-25T12:03:22.188389",
     "exception": false,
     "start_time": "2023-07-25T12:03:09.819716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zlib\n",
    "import base64\n",
    "\n",
    "from pycocotools import _mask as coco_mask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import measure\n",
    "from skimage.draw import polygon\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db2d99",
   "metadata": {
    "papermill": {
     "duration": 0.008425,
     "end_time": "2023-07-25T12:03:22.208056",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.199631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities\n",
    "\n",
    "Here are some useful utilities that help in debugging, model training, and model evalutation.\n",
    "\n",
    "I wrote a custom version of mAP for fun so use at your own risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6763fe1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.230665Z",
     "iopub.status.busy": "2023-07-25T12:03:22.229945Z",
     "iopub.status.idle": "2023-07-25T12:03:22.573856Z",
     "shell.execute_reply": "2023-07-25T12:03:22.572668Z"
    },
    "papermill": {
     "duration": 0.357579,
     "end_time": "2023-07-25T12:03:22.576460",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.218881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return torch.count_nonzero(preds == labels) / preds.shape[0]\n",
    "\n",
    "\n",
    "def memory_usage_stats(model, optimizer, batch_size=1, device='cuda'):\n",
    "    print(f'Starting memory: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    model.to(device)\n",
    "    print(f'After model sent to {device}: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    for i in range(3):\n",
    "        sample = torch.randn((batch_size, 3, 512, 512))\n",
    "        print(f'Step {i}')\n",
    "        before = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        out = model(sample.to(device)).sum()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After forward pass: {after}')\n",
    "        print(f'Memory used by forward pass: {after - before}')\n",
    "        out.backward()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After backward pass: {after}')\n",
    "        optimizer.step()\n",
    "        print(f'After optimizer step: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def memory_usage_stats_grad_scaler(model, optimizer, batch_size=1, device='cuda'):\n",
    "    if device != 'cuda':\n",
    "        print('This function requires device to be \"cuda\".')\n",
    "        return\n",
    "    print(f'Starting memory: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    model.to(device)\n",
    "    print(f'After model sent to {device}: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for i in range(3):\n",
    "        sample = torch.randn((batch_size, 3, 512, 512))\n",
    "        print(f'Step {i}')\n",
    "        before = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(sample.to(device)).sum()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After forward pass: {after}')\n",
    "        print(f'Memory used by forward pass: {after - before}')\n",
    "        scaler.scale(out).backward()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After backward pass: {after}')\n",
    "        scaler.step(optimizer)\n",
    "        print(f'After optimizer step: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "        scaler.update()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def average_precision(prediction, target, iou_threshold=0.6):\n",
    "    '''\n",
    "    Computes the average precision (AP) for an instance segmentation task on a single image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : np.array\n",
    "        Boolean prediction mask\n",
    "    target : np.array\n",
    "        Boolean ground-truth mask\n",
    "    iou_threshold : float, optional\n",
    "        Threshold at which to count predictions as positive predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AP : float\n",
    "        Average precision score\n",
    "    '''\n",
    "    pred_regions, pred_region_count = measure.label(prediction, return_num=True)\n",
    "    target_regions, target_region_count = measure.label(target, return_num=True)\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for p in range(1, pred_region_count + 1):\n",
    "        pred_region_mask = pred_regions == p\n",
    "        max_iou = 0.0\n",
    "        for t in range(1, target_region_count + 1):\n",
    "            target_region_mask = target_regions == t\n",
    "            # Compute IoU this region pair\n",
    "            intersection = np.logical_and(pred_region_mask, target_region_mask)\n",
    "            union = np.logical_or(pred_region_mask, target_region_mask)\n",
    "            intersection_count = np.count_nonzero(intersection)\n",
    "            union_count = np.count_nonzero(union)\n",
    "            if intersection_count > 0 and union_count > 0:\n",
    "                iou = intersection_count / union_count\n",
    "                max_iou = max(max_iou, iou)\n",
    "        if max_iou > iou_threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    if (true_positives == 0 and false_positives == 0) or target_region_count == 0:\n",
    "        return 0.0\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / target_region_count\n",
    "\n",
    "    return precision * recall\n",
    "\n",
    "\n",
    "def mAP(predictions, targets, iou_threshold=0.6):\n",
    "    averages = 0.0\n",
    "    predictions = predictions.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    for p, t in zip(predictions, targets):\n",
    "        averages += average_precision(p, t, iou_threshold=iou_threshold)\n",
    "    return averages / len(predictions)\n",
    "\n",
    "\n",
    "def logits_to_blood_vessel_mask(logits):\n",
    "    return (torch.argmax(torch.softmax(logits, dim=1), dim=1) == 2).type(torch.long)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, model, train_loader, loss_func, optimizer, writer=None, device='cpu', **kwargs):\n",
    "    total_loss = 0.0\n",
    "    data_size = len(train_loader)\n",
    "    model.train()\n",
    "    for d in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = d['image']\n",
    "        y = d['mask']\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_func(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if writer:\n",
    "            with torch.no_grad():\n",
    "                predictions = logits_to_blood_vessel_mask(logits)\n",
    "                blood_vessel_gt = (y == 2).type(torch.long)\n",
    "                mAP_value = mAP(predictions, blood_vessel_gt, **kwargs)\n",
    "                writer.add_scalar(\"Loss/train\", loss.item(), global_step=epoch)\n",
    "                writer.add_scalar(\"mAP/train\", mAP_value, global_step=epoch)\n",
    "    \n",
    "    return total_loss / data_size\n",
    "\n",
    "\n",
    "def validate_one_epoch(epoch, model, valid_loader, loss_func, writer, device='cpu', **kwargs):\n",
    "    model.eval()\n",
    "    data_size = len(valid_loader)\n",
    "    total_loss = 0.0\n",
    "    total_mAP = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(valid_loader):\n",
    "            x = d['image']\n",
    "            y = d['mask']\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_func(logits, y)\n",
    "            predictions = logits_to_blood_vessel_mask(logits)\n",
    "            blood_vessel_gt = (y == 2).type(torch.long)\n",
    "            mAP_value = mAP(predictions, blood_vessel_gt, **kwargs)\n",
    "            writer.add_scalar(\"Loss/valid\", loss.item(), global_step=epoch)\n",
    "            writer.add_scalar(\"mAP/valid\", mAP_value, global_step=epoch)\n",
    "            total_loss += loss.item()\n",
    "            total_mAP += mAP_value\n",
    "\n",
    "    return total_loss / data_size, total_mAP / data_size\n",
    "    \n",
    "\n",
    "\n",
    "def oid_mask_encoding(mask):\n",
    "    '''\n",
    "    Taken from https://gist.github.com/pculliton/209398a2a52867580c6103e25e55d93c\n",
    "    using the competition page\n",
    "    '''\n",
    "    # check input mask --\n",
    "    if mask.dtype != bool:\n",
    "        raise ValueError(\n",
    "            \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n",
    "            mask.dtype)\n",
    "\n",
    "    mask = np.squeeze(mask)\n",
    "    if len(mask.shape) != 2:\n",
    "        raise ValueError(\n",
    "            \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n",
    "            mask.shape)\n",
    "\n",
    "    # convert input mask to expected COCO API input --\n",
    "    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "    mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "    mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "    # RLE encode mask --\n",
    "    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "    # compress and base64 encoding --\n",
    "    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "    base64_str = base64.b64encode(binary_str)\n",
    "    return base64_str\n",
    "\n",
    "\n",
    "def kaggle_prediction(image_id, logits):\n",
    "    '''\n",
    "    Encodes the raw output of the model into the submission format\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_id : str\n",
    "        Identifier of the predicted image\n",
    "    logits : torch.tensor\n",
    "        Raw output of the model with shape (3, 512, 512)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Submission entry with the headers (id, height, width, prediction_string)\n",
    "    '''\n",
    "\n",
    "    # keep raw prediction probabilities\n",
    "    preds = torch.softmax(logits, dim=0)\n",
    "\n",
    "    # convert blood vessel predictions to the mask\n",
    "    prediction = logits_to_blood_vessel_mask(logits.unsqueeze(0)).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # label predicted connected components\n",
    "    pred_regions, pred_region_count = measure.label(prediction, return_num=True)\n",
    "\n",
    "    prediction_string = ''\n",
    "    for p in range(1, pred_region_count + 1):\n",
    "        # get the coordinates of the current region\n",
    "        x, y = np.where(pred_regions == p)\n",
    "\n",
    "        # separate the binary mask\n",
    "        mask = np.zeros_like(pred_regions, dtype=bool)\n",
    "        mask[(x, y)] = True\n",
    "\n",
    "        # encode the separated mask\n",
    "        encoding = oid_mask_encoding(mask)\n",
    "\n",
    "        # average the prediction probabilities of the current region\n",
    "        region_prob = torch.mean(preds[2, x, y])\n",
    "\n",
    "        prediction_string += f'0 {region_prob.item()} {encoding.decode(\"utf-8\")} '\n",
    "    \n",
    "    return {\n",
    "        'id': image_id,\n",
    "        'height': 512,\n",
    "        'width': 512,\n",
    "        'prediction_string': prediction_string.strip()\n",
    "    }\n",
    "\n",
    "\n",
    "def save_model_checkpoint(path, epoch, model, optimizer, loss, scheduler=None):\n",
    "    checkpoint_dict = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'training_loss': loss,\n",
    "    }\n",
    "    if scheduler:\n",
    "        checkpoint_dict['scheduler_state_dict'] = scheduler.state_dict()\n",
    "    else:\n",
    "        checkpoint_dict['scheduler_state_dict'] = None\n",
    "    torch.save(checkpoint_dict, path)\n",
    "\n",
    "\n",
    "def load_model_checkpoint(path, model, optimizer=None, scheduler=None):\n",
    "    checkpoint = torch.load(path)\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d27729",
   "metadata": {
    "papermill": {
     "duration": 0.006372,
     "end_time": "2023-07-25T12:03:22.589593",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.583221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Architecture\n",
    "\n",
    "Here is the UNet architecture implementation.\n",
    "\n",
    "See the UNet paper for more information: https://arxiv.org/pdf/1505.04597v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e5561a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.603407Z",
     "iopub.status.busy": "2023-07-25T12:03:22.603096Z",
     "iopub.status.idle": "2023-07-25T12:03:22.619118Z",
     "shell.execute_reply": "2023-07-25T12:03:22.618200Z"
    },
    "papermill": {
     "duration": 0.025527,
     "end_time": "2023-07-25T12:03:22.621225",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.595698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        '''\n",
    "        Note: We pad the convolutions here since we know the input image size is always 512x512.\n",
    "        Otherwise we would do the \"Overlap-tile strategy\" presented in the u-net paper.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, xx):\n",
    "        return self.model(xx)\n",
    "\n",
    "\n",
    "class UNet2d(nn.Module):\n",
    "    ''' Paper: https://arxiv.org/pdf/1505.04597v1.pdf '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Down\n",
    "        self.conv1 = ConvBlock(3, 64)\n",
    "        self.conv2 = ConvBlock(64, 128)\n",
    "        self.conv3 = ConvBlock(128, 256)\n",
    "        self.conv4 = ConvBlock(256, 512)\n",
    "        self.conv5 = ConvBlock(512, 1024)\n",
    "\n",
    "        # Up\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv6 = ConvBlock(1024, 512)\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv7 = ConvBlock(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv8 = ConvBlock(256, 128)\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv9 = ConvBlock(128, 64)\n",
    "        self.conv_out = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, xx):\n",
    "        # Down\n",
    "        x1 = self.conv1(xx)\n",
    "        xx = self.max_pool(x1)\n",
    "        x2 = self.conv2(xx)\n",
    "        xx = self.max_pool(x2)\n",
    "        x3 = self.conv3(xx)\n",
    "        xx = self.max_pool(x3)\n",
    "        x4 = self.conv4(xx)\n",
    "        xx = self.max_pool(x4)\n",
    "        xx = self.conv5(xx)\n",
    "\n",
    "        # Up\n",
    "        xx = self.upconv1(xx)\n",
    "        xx = torch.cat((x4, xx), dim=1)\n",
    "        xx = self.conv6(xx)\n",
    "        xx = self.upconv2(xx)\n",
    "        xx = torch.cat((x3, xx), dim=1)\n",
    "        xx = self.conv7(xx)\n",
    "        xx = self.upconv3(xx)\n",
    "        xx = torch.cat((x2, xx), dim=1)\n",
    "        xx = self.conv8(xx)\n",
    "        xx = self.upconv4(xx)\n",
    "        xx = torch.cat((x1, xx), dim=1)\n",
    "        xx = self.conv9(xx)\n",
    "        return self.conv_out(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d3570",
   "metadata": {
    "papermill": {
     "duration": 0.00596,
     "end_time": "2023-07-25T12:03:22.633551",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.627591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Here is the dataset that loads the images, annotations, and peforms data augmentation.\n",
    "\n",
    "We allow for the prediction of the background (label 0), glomerulus (label 1), and blood vessels (label 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171776d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.647401Z",
     "iopub.status.busy": "2023-07-25T12:03:22.647129Z",
     "iopub.status.idle": "2023-07-25T12:03:22.667481Z",
     "shell.execute_reply": "2023-07-25T12:03:22.666679Z"
    },
    "papermill": {
     "duration": 0.029654,
     "end_time": "2023-07-25T12:03:22.669557",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.639903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HuBMAP(Dataset):\n",
    "    ''' Training dataset for the HuBMAP Kaggle Competition '''\n",
    "    def __init__(self, data_dir=os.path.join('.', 'data'), submission=False, include_unsure=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_ids = []\n",
    "        self.images = []\n",
    "        self.masks = [] # target structure\n",
    "\n",
    "        if not submission:\n",
    "            # Load in the training labels\n",
    "            with open(os.path.join(data_dir, 'polygons.jsonl'), 'r') as polygons_file:\n",
    "                polygons = list(polygons_file)\n",
    "            self.polygons = [json.loads(p) for p in polygons]\n",
    "\n",
    "            # Load all of the training images and annotations into memory\n",
    "            self.img_size = 512\n",
    "            print(\"Loading in images and converting annotations to polygon masks...\")\n",
    "            for poly in tqdm(self.polygons):\n",
    "                id = poly['id']\n",
    "                # Get image using id\n",
    "                image = Image.open(os.path.join(data_dir, 'train', f'{id}.tif'))\n",
    "                self.image_ids.append(id)\n",
    "                self.images.append(image)\n",
    "\n",
    "                # Get all of the different annotations for this image\n",
    "                ## A single image can have multiple annotations for each type\n",
    "                blood_vessel_coords = []\n",
    "                glomerulus_coords = []\n",
    "                unsure_coords = []\n",
    "                ## Load in the type and coordinates for each annotation\n",
    "                annotations = poly['annotations']\n",
    "                for ann in annotations:\n",
    "                    type = ann['type']\n",
    "                    assert len(ann['coordinates']) <= 1\n",
    "                    coordinates = ann['coordinates'][0]\n",
    "                    row_indices = [c[0] for c in coordinates] \n",
    "                    col_indices = [c[1] for c in coordinates]\n",
    "                    row_indices, col_indices = polygon(row_indices, col_indices, (self.img_size, self.img_size))\n",
    "                    coordinates = list(zip(row_indices, col_indices))\n",
    "                    if type == 'blood_vessel':\n",
    "                        blood_vessel_coords.append(coordinates)\n",
    "                    elif type == 'glomerulus':\n",
    "                        glomerulus_coords.append(coordinates)\n",
    "                    else:\n",
    "                        unsure_coords.append(coordinates) \n",
    "                mask = self.coordinates_to_mask([item for sublist in glomerulus_coords for item in sublist])\n",
    "                if include_unsure and unsure_coords is not None and len(unsure_coords) > 0:\n",
    "                    uns_coords = torch.tensor([item for sublist in unsure_coords for item in sublist])\n",
    "                    mask[uns_coords[:, 1], uns_coords[:, 0]] = 2\n",
    "                if blood_vessel_coords is not None and len(blood_vessel_coords) > 0:\n",
    "                    bv_coords = torch.tensor([item for sublist in blood_vessel_coords for item in sublist])\n",
    "                    # coordinates are (x, y) like a grid\n",
    "                    mask[bv_coords[:, 1], bv_coords[:, 0]] = 2\n",
    "                self.masks.append(mask)\n",
    "            print(\"Done.\")\n",
    "            # Set up image transformations\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((512, 512), antialias=True)\n",
    "            ])\n",
    "        else:\n",
    "            for image_file in os.listdir(os.path.join(data_dir, 'test')):\n",
    "                image = Image.open(os.path.join(data_dir, 'test', image_file))\n",
    "                id = image_file.split('.')[0]\n",
    "                self.image_ids.append(id)\n",
    "                self.images.append(image)\n",
    "                self.masks.append(torch.zeros((512, 512)))\n",
    "\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((512, 512), antialias=True)\n",
    "            ])\n",
    "\n",
    "    def coordinates_to_mask(self, coordinates):\n",
    "        if coordinates is None or coordinates == []:\n",
    "            return torch.zeros((self.img_size, self.img_size), dtype=torch.long)\n",
    "        coords = torch.tensor(coordinates)\n",
    "        mask = torch.zeros((self.img_size, self.img_size), dtype=torch.bool)\n",
    "        # coordinates are (x, y) like a grid\n",
    "        mask[coords[:, 1], coords[:, 0]] = True\n",
    "        return mask.type(torch.long)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'id': self.image_ids[i],\n",
    "            'image': self.transforms(self.images[i]),\n",
    "            'mask': self.masks[i],\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61800df3",
   "metadata": {
    "papermill": {
     "duration": 0.005905,
     "end_time": "2023-07-25T12:03:22.681805",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.675900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CUDA Memory Usage\n",
    "\n",
    "It's important to see how much memory our model is using during training. Since this is a fairly large model with a large input (512x512 images), we need to know how large a batch size we can use on a single device. This will make training easier since our batch normalization will have better statistics the larger the batch size we use.\n",
    "\n",
    "\n",
    "Using a very low batch size could actually decrease performance when using batch normalization since it's statistics will be very noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf1601e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.694974Z",
     "iopub.status.busy": "2023-07-25T12:03:22.694709Z",
     "iopub.status.idle": "2023-07-25T12:03:22.698579Z",
     "shell.execute_reply": "2023-07-25T12:03:22.697659Z"
    },
    "papermill": {
     "duration": 0.012762,
     "end_time": "2023-07-25T12:03:22.700609",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.687847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = UNet2d()\n",
    "#optimizer = optim.Adam(model.parameters())\n",
    "#memory_usage_stats(model, optimizer, batch_size=9, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f80e0",
   "metadata": {
    "papermill": {
     "duration": 0.006014,
     "end_time": "2023-07-25T12:03:22.712745",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.706731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training\n",
    "\n",
    "Here is where we actually train the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5484fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T12:03:22.726192Z",
     "iopub.status.busy": "2023-07-25T12:03:22.725935Z",
     "iopub.status.idle": "2023-07-25T22:03:25.595240Z",
     "shell.execute_reply": "2023-07-25T22:03:25.592322Z"
    },
    "papermill": {
     "duration": 36002.956718,
     "end_time": "2023-07-25T22:03:25.675541",
     "exception": false,
     "start_time": "2023-07-25T12:03:22.718823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in images and converting annotations to polygon masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1633/1633 [04:47<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [9:55:05<00:00, 178.53s/it]\n"
     ]
    }
   ],
   "source": [
    "os.mkdir('checkpoints')\n",
    "# PARAMETERS\n",
    "RUN_NAME = 'release1_run1'\n",
    "BATCH_SIZE = 9\n",
    "LR = 3e-4\n",
    "WD = 0.0\n",
    "MOMENTUM = 0.0\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "VALID_STEP = 5\n",
    "RNG = 32\n",
    "EPOCH_START = 0\n",
    "EPOCH_END = 200\n",
    "CHECKPOINT_STEP = 5\n",
    "CHECKPOINT_LOAD_PATH = None\n",
    "CHECKPOINT_SAVE_PATH = os.path.join('checkpoints', f'{RUN_NAME}.pt')\n",
    "INCLUDE_UNSURE = True\n",
    "\n",
    "torch.manual_seed(RNG)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "dataset = HuBMAP(data_dir=os.path.join('/kaggle', 'input', 'hubmap-hacking-the-human-vasculature'),\n",
    "                 include_unsure=INCLUDE_UNSURE)\n",
    "generator = torch.Generator().manual_seed(RNG)\n",
    "train_data, valid_data = random_split(dataset, [0.9, 0.1], generator=generator)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "valid_loader = DataLoader(valid_data, batch_size=1, shuffle=False, pin_memory=False)\n",
    "model = UNet2d().to(DEVICE)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max')\n",
    "scheduler = None\n",
    "if CHECKPOINT_LOAD_PATH:\n",
    "    EPOCH_START = load_model_checkpoint(CHECKPOINT_LOAD_PATH, model, optimizer, scheduler=scheduler) \n",
    "\n",
    "for e in tqdm(range(EPOCH_START + 1, EPOCH_END + 1)):\n",
    "    if e % VALID_STEP == 0:\n",
    "        loss = train_one_epoch(e, model, train_loader, loss_func, optimizer, writer=writer, device=DEVICE)\n",
    "        val_loss, val_metric = validate_one_epoch(e, model, valid_loader, loss_func, writer, device=DEVICE)\n",
    "        if scheduler:\n",
    "            scheduler.step(val_metric)\n",
    "    else:\n",
    "        loss = train_one_epoch(e, model, train_loader, loss_func, optimizer, device=DEVICE)\n",
    "    if e % CHECKPOINT_STEP == 0:\n",
    "        save_model_checkpoint(CHECKPOINT_SAVE_PATH, e, model, optimizer, loss, scheduler=scheduler)\n",
    "    #writer.add_scalar('gpu_memory_usage', torch.cuda.memory_allocated(DEVICE), global_step=e)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba33d67",
   "metadata": {
    "papermill": {
     "duration": 0.079246,
     "end_time": "2023-07-25T22:03:25.833491",
     "exception": false,
     "start_time": "2023-07-25T22:03:25.754245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission\n",
    "Here is where we predict on the test images for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30224af9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T22:03:25.995729Z",
     "iopub.status.busy": "2023-07-25T22:03:25.994689Z",
     "iopub.status.idle": "2023-07-25T22:03:26.146476Z",
     "shell.execute_reply": "2023-07-25T22:03:26.145420Z"
    },
    "papermill": {
     "duration": 0.237007,
     "end_time": "2023-07-25T22:03:26.149463",
     "exception": false,
     "start_time": "2023-07-25T22:03:25.912456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "RUN_NAME = 'release1_run1'\n",
    "BATCH_SIZE = 1\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "RNG = 32\n",
    "CHECKPOINT_LOAD_PATH = os.path.join('checkpoints', f'{RUN_NAME}.pt')\n",
    "\n",
    "torch.manual_seed(RNG)\n",
    "\n",
    "dataset = HuBMAP(data_dir=os.path.join('/kaggle', 'input', 'hubmap-hacking-the-human-vasculature'), submission=True)\n",
    "#generator = torch.Generator().manual_seed(RNG)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=False)\n",
    "#model = UNet2d().to(DEVICE)\n",
    "#if CHECKPOINT_LOAD_PATH:\n",
    "    #EPOCH_START = load_model_checkpoint(CHECKPOINT_LOAD_PATH, model)\n",
    "model.eval()\n",
    "\n",
    "submission_entries = pd.DataFrame(columns=['id', 'height', 'width', 'prediction_string'])\n",
    "for d in loader:\n",
    "    id = d['id']\n",
    "    x = d['image']\n",
    "    x = x.to(DEVICE)\n",
    "    logits = model(x)\n",
    "    for i in range(logits.shape[0]):\n",
    "        submission = kaggle_prediction(id, logits[i, :, :, :])\n",
    "        submission_df = pd.DataFrame(submission)\n",
    "        submission_entries = pd.concat((submission_entries, submission_df), axis=0)\n",
    "\n",
    "submission_entries.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36064.746083,
   "end_time": "2023-07-25T22:03:29.897373",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-25T12:02:25.151290",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
