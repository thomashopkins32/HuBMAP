{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuBMAP Release 1\n",
    "Thomas Hopkins\n",
    "\n",
    "## Overview\n",
    "\n",
    "Here is the first attempt at training a UNet for the HuBMAP competition.\n",
    "\n",
    "The code for this notebook is ported from my GitHub repository here: https://github.com/thomashopkins32/HuBMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import measure\n",
    "from skimage.draw import polygon\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard.notebook\n",
    "#%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Here are some useful utilities that help in debugging, model training, and model evalutation.\n",
    "\n",
    "I wrote a custom version of mAP for fun so use at your own risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return torch.count_nonzero(preds == labels) / preds.shape[0]\n",
    "\n",
    "\n",
    "def memory_usage_stats(model, optimizer, batch_size=1, device='cuda'):\n",
    "    print(f'Starting memory: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    model.to(device)\n",
    "    print(f'After model sent to {device}: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    for i in range(3):\n",
    "        sample = torch.randn((batch_size, 3, 512, 512))\n",
    "        print(f'Step {i}')\n",
    "        before = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        out = model(sample.to(device)).sum()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After forward pass: {after}')\n",
    "        print(f'Memory used by forward pass: {after - before}')\n",
    "        out.backward()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After backward pass: {after}')\n",
    "        optimizer.step()\n",
    "        print(f'After optimizer step: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def memory_usage_stats_grad_scaler(model, optimizer, batch_size=1, device='cuda'):\n",
    "    if device != 'cuda':\n",
    "        print('This function requires device to be \"cuda\".')\n",
    "        return\n",
    "    print(f'Starting memory: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    model.to(device)\n",
    "    print(f'After model sent to {device}: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for i in range(3):\n",
    "        sample = torch.randn((batch_size, 3, 512, 512))\n",
    "        print(f'Step {i}')\n",
    "        before = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(sample.to(device)).sum()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After forward pass: {after}')\n",
    "        print(f'Memory used by forward pass: {after - before}')\n",
    "        scaler.scale(out).backward()\n",
    "        after = torch.cuda.memory_allocated(device) * 1e-6\n",
    "        print(f'After backward pass: {after}')\n",
    "        scaler.step(optimizer)\n",
    "        print(f'After optimizer step: {torch.cuda.memory_allocated(device) * 1e-6}')\n",
    "        scaler.update()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def average_precision(prediction, target, iou_threshold=0.6):\n",
    "    '''\n",
    "    Computes the average precision (AP) for an instance segmentation task on a single image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : np.array\n",
    "        Boolean prediction mask\n",
    "    target : np.array\n",
    "        Boolean ground-truth mask\n",
    "    iou_threshold : float, optional\n",
    "        Threshold at which to count predictions as positive predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AP : float\n",
    "        Average precision score\n",
    "    '''\n",
    "    pred_regions, pred_region_count = measure.label(prediction, return_num=True)\n",
    "    target_regions, target_region_count = measure.label(target, return_num=True)\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for p in range(1, pred_region_count + 1):\n",
    "        pred_region_mask = pred_regions == p\n",
    "        max_iou = 0.0\n",
    "        for t in range(1, target_region_count + 1):\n",
    "            target_region_mask = target_regions == t\n",
    "            # Compute IoU this region pair\n",
    "            intersection = np.logical_and(pred_region_mask, target_region_mask)\n",
    "            union = np.logical_or(pred_region_mask, target_region_mask)\n",
    "            intersection_count = np.count_nonzero(intersection)\n",
    "            union_count = np.count_nonzero(union)\n",
    "            if intersection_count > 0 and union_count > 0:\n",
    "                iou = intersection_count / union_count\n",
    "                max_iou = max(max_iou, iou)\n",
    "        if max_iou > iou_threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    if (true_positives == 0 and false_positives == 0) or target_region_count == 0:\n",
    "        return 0.0\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / target_region_count\n",
    "\n",
    "    return precision * recall\n",
    "\n",
    "\n",
    "def mAP(predictions, targets, iou_threshold=0.6):\n",
    "    averages = 0.0\n",
    "    predictions = predictions.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    for p, t in zip(predictions, targets):\n",
    "        averages += average_precision(p, t, iou_threshold=iou_threshold)\n",
    "    return averages / len(predictions)\n",
    "\n",
    "\n",
    "def logits_to_mask(logits):\n",
    "    # TODO: update with only blood vessels\n",
    "    return torch.argmax(torch.softmax(logits, dim=1), dim=1).type(torch.long)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, model, train_loader, loss_func, optimizer, writer=None, device='cpu', **kwargs):\n",
    "    model.train()\n",
    "    for d in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = d['image']\n",
    "        y = d['mask']\n",
    "        x = x.to(device)\n",
    "        y = y.long().to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_func(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if writer:\n",
    "            with torch.no_grad():\n",
    "                predictions = logits_to_mask(logits)\n",
    "                mAP_value = mAP(predictions, y, **kwargs)\n",
    "                writer.add_scalar(\"Loss/train\", loss.item(), global_step=epoch)\n",
    "                writer.add_scalar(\"mAP/train\", mAP_value, global_step=epoch)\n",
    "\n",
    "\n",
    "def validate_one_epoch(epoch, model, valid_loader, loss_func, writer, device='cpu', **kwargs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(valid_loader):\n",
    "            x = d['image']\n",
    "            y = d['mask']\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_func(logits, y)\n",
    "            predictions = logits_to_mask(logits)\n",
    "            mAP_value = mAP(predictions, y, **kwargs)\n",
    "            writer.add_scalar(\"Loss/valid\", loss.item(), global_step=epoch)\n",
    "            writer.add_scalar(\"mAP/valid\", mAP_value, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Here is the UNet architecture implementation.\n",
    "\n",
    "See the UNet paper for more information: https://arxiv.org/pdf/1505.04597v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        '''\n",
    "        Note: We pad the convolutions here since we know the input image size is always 512x512.\n",
    "        Otherwise we would do the \"Overlap-tile strategy\" presented in the u-net paper.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, xx):\n",
    "        return self.model(xx)\n",
    "\n",
    "\n",
    "class UNet2d(nn.Module):\n",
    "    ''' Paper: https://arxiv.org/pdf/1505.04597v1.pdf '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Down\n",
    "        self.conv1 = ConvBlock(3, 64)\n",
    "        self.conv2 = ConvBlock(64, 128)\n",
    "        self.conv3 = ConvBlock(128, 256)\n",
    "        self.conv4 = ConvBlock(256, 512)\n",
    "        self.conv5 = ConvBlock(512, 1024)\n",
    "\n",
    "        # Up\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv6 = ConvBlock(1024, 512)\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv7 = ConvBlock(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv8 = ConvBlock(256, 128)\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv9 = ConvBlock(128, 64)\n",
    "        self.conv_out = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, xx):\n",
    "        # Down\n",
    "        x1 = self.conv1(xx)\n",
    "        xx = self.max_pool(x1)\n",
    "        x2 = self.conv2(xx)\n",
    "        xx = self.max_pool(x2)\n",
    "        x3 = self.conv3(xx)\n",
    "        xx = self.max_pool(x3)\n",
    "        x4 = self.conv4(xx)\n",
    "        xx = self.max_pool(x4)\n",
    "        xx = self.conv5(xx)\n",
    "\n",
    "        # Up\n",
    "        xx = self.upconv1(xx)\n",
    "        xx = torch.cat((x4, xx), dim=1)\n",
    "        xx = self.conv6(xx)\n",
    "        xx = self.upconv2(xx)\n",
    "        xx = torch.cat((x3, xx), dim=1)\n",
    "        xx = self.conv7(xx)\n",
    "        xx = self.upconv3(xx)\n",
    "        xx = torch.cat((x2, xx), dim=1)\n",
    "        xx = self.conv8(xx)\n",
    "        xx = self.upconv4(xx)\n",
    "        xx = torch.cat((x1, xx), dim=1)\n",
    "        xx = self.conv9(xx)\n",
    "        return self.conv_out(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Here is the dataset that loads the images, annotations, and peforms data augmentation.\n",
    "\n",
    "We allow for the prediction of the background (label 0), glomerulus (label 1), and blood vessels (label 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class HuBMAP(Dataset):\n",
    "    ''' Training dataset for the HuBMAP Kaggle Competition '''\n",
    "    def __init__(self, data_dir=os.path.join('.', 'data'), include_unsure=False):\n",
    "        self.data_dir = data_dir\n",
    "        # Load in the training labels\n",
    "        with open(os.path.join(data_dir, 'polygons.jsonl'), 'r') as polygons_file:\n",
    "            polygons = list(polygons_file)\n",
    "        self.polygons = [json.loads(p) for p in polygons]\n",
    "\n",
    "        # Load all of the training images and annotations into memory\n",
    "        self.img_size = 512\n",
    "        self.images = []\n",
    "        self.masks = [] # target structure\n",
    "        print(\"Loading in images and converting annotations to polygon masks...\")\n",
    "        for poly in tqdm(self.polygons):\n",
    "            id = poly['id']\n",
    "            # Get image using id\n",
    "            image = Image.open(os.path.join(data_dir, 'train', f'{id}.tif'))\n",
    "            self.images.append(image)\n",
    "\n",
    "            # Get all of the different annotations for this image\n",
    "            ## A single image can have multiple annotations for each type\n",
    "            blood_vessel_coords = []\n",
    "            glomerulus_coords = []\n",
    "            unsure_coords = []\n",
    "            ## Load in the type and coordinates for each annotation\n",
    "            annotations = poly['annotations']\n",
    "            for ann in annotations:\n",
    "                type = ann['type']\n",
    "                assert len(ann['coordinates']) <= 1\n",
    "                coordinates = ann['coordinates'][0]\n",
    "                row_indices = [c[0] for c in coordinates] \n",
    "                col_indices = [c[1] for c in coordinates]\n",
    "                row_indices, col_indices = polygon(row_indices, col_indices, (self.img_size, self.img_size))\n",
    "                coordinates = list(zip(row_indices, col_indices))\n",
    "                if type == 'blood_vessel':\n",
    "                    blood_vessel_coords.append(coordinates)\n",
    "                elif type == 'glomerulus':\n",
    "                    glomerulus_coords.append(coordinates)\n",
    "                else:\n",
    "                    unsure_coords.append(coordinates) \n",
    "            mask = self.coordinates_to_mask([item for sublist in glomerulus_coords for item in sublist])\n",
    "            if include_unsure and unsure_coords is not None and len(unsure_coords) > 0:\n",
    "                uns_coords = torch.tensor([item for sublist in unsure_coords for item in sublist])\n",
    "                mask[uns_coords[:, 1], uns_coords[:, 0]] = 2\n",
    "            bv_coords = torch.tensor([item for sublist in blood_vessel_coords for item in sublist])\n",
    "            # coordinates are (x, y) like a grid\n",
    "            mask[bv_coords[:, 1], bv_coords[:, 0]] = 2\n",
    "            self.masks.append(mask)\n",
    "        print(\"Done.\")\n",
    "        # Set up image transformations\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((512, 512), antialias=True)\n",
    "        ])\n",
    "\n",
    "    def coordinates_to_mask(self, coordinates):\n",
    "        if coordinates is None or coordinates == []:\n",
    "            return torch.zeros((self.img_size, self.img_size), dtype=torch.bool)\n",
    "        coords = torch.tensor(coordinates)\n",
    "        mask = torch.zeros((self.img_size, self.img_size), dtype=torch.bool)\n",
    "        # coordinates are (x, y) like a grid\n",
    "        mask[coords[:, 1], coords[:, 0]] = True\n",
    "        return mask.type(torch.long)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'image': self.transforms(self.images[i]),\n",
    "            'mask': self.masks[i],\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2d()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "memory_usage_stats(model, optimizer, 3, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage_stats_grad_scaler(model, optimizer, 4, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "WD = 0.0\n",
    "MOMENTUM = 0.0\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "VALID_STEP = 5\n",
    "RNG = 32\n",
    "EPOCHS = 50\n",
    "INCLUDE_UNSURE = True\n",
    "\n",
    "torch.manual_seed(RNG)\n",
    "\n",
    "writer = SummaryWriter(max_queue=1000, flush_secs=300)\n",
    "dataset = HuBMAP(include_unsure=INCLUDE_UNSURE)\n",
    "generator = torch.Generator().manual_seed(RNG)\n",
    "train_data, valid_data = random_split(dataset, [0.9, 0.1], generator=generator)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "valid_loader = DataLoader(valid_data, batch_size=1, shuffle=False, pin_memory=False)\n",
    "model = UNet2d().to(DEVICE)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for e in tqdm(range(EPOCHS)):\n",
    "    if e % VALID_STEP == 0:\n",
    "        train_one_epoch(e, model, train_loader, loss_func, optimizer, writer=writer, device=DEVICE)\n",
    "        validate_one_epoch(e, model, valid_loader, loss_func, writer, device=DEVICE)\n",
    "    else:\n",
    "        train_one_epoch(e, model, train_loader, loss_func, optimizer, device=DEVICE)\n",
    "    writer.add_scalar('gpu_memory_usage', torch.cuda.memory_allocated(DEVICE), global_step=e)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
